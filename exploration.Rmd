---
title: "Literature review"
output: html_document
---

```{r,echo=FALSE, message=FALSE, results='hide', warning=FALSE}
# Loading libraries
library(ggplot2)
library(splitstackshape)
library(igraph)
library(knitr)

# Set ggplot theme
theme_set(theme_minimal(12))
```


```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
# Loading and preparing data

# Call cleaning2.R to process the data in the input folder and
# save processed files to output folder
source("cleaning2.R", chdir = T)

# Load yearly publication data
years <- read.table("analyze.csv", sep = ";", header = T)

# Helper function to remove leading and trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

# Fixing variable types
# (Manual conversion to chararcters could be avoided by setting
# stringsAsFactors=FALSE in read.delim() function in cleaning2.R. Oh well.)
literature$AuthorFullName <- as.character(literature$AuthorFullName)
literatureByAuthor$AuthorFullName <- as.character(literatureByAuthor$AuthorFullName)
literatureByKeywords$AuthorFullName <- as.character(literatureByKeywords$AuthorFullName)
literatureByCategory$AuthorFullName <- as.character(literatureByCategory$AuthorFullName)

literature$Abstract <- as.character(literature$Abstract)
literature$DocumentTitle <- as.character(literature$DocumentTitle)

literature$YearPublished <- as.numeric(as.character(literature$YearPublished))

literature$CitedReferences <- as.character(literature$CitedReferences)

literature$TimesCited <- as.numeric(literature$TimesCited)
literatureByAuthor$TimesCited <- as.numeric(literatureByAuthor$TimesCited)
literatureByKeywords$TimesCited <- as.numeric(literatureByKeywords$TimesCited)
literatureByCategory$TimesCited <- as.numeric(literatureByCategory$TimesCited)

literature$AuthorKeywords <- as.character(literature$AuthorKeywords)

literatureByKeywords$AuthorKeywords <- as.character(literatureByKeywords$AuthorKeywords)
```
This report provides an analysis on the records downloaded from [Web of Science](http://webofscience.com). The analysis identifies the important authors, journals, and keywords in the dataset based on the number of occurences and citation counts. A citation network of the provided records is created and used to identify the important papers according to their in-degree, total citation count and PageRank scores. The analysis finds also often-cited references that were not included in the original dataset downloaded from the Web of Science.

Reports can be generated by using the [online analysis service](http://hammer.nailsproject.net/), and the source code is available at [GitHub](https://github.com/aknutas/nails). Instructions and links to tutorial videos can be found at the [project page](https://aknutas.github.io/nails/). For further details see the following article: [Knutas, A., Hajikhani, A., Salminen, J., Ikonen, J., Porras, J., 2015. Cloud-Based Bibliometric Analysis Service for Systematic Mapping Studies. CompSysTech 2015](http://www.codecamp.fi/lib/exe/fetch.php/wiki/nails-compsystech2015-preprint.pdf). Please cite our research paper on bibliometrics if you publish the analysis results. Use is otherwise free.

The analysed dataset consist of `r nrow(literature)` records with `r ncol(literature)` variables. More information about the variables can be found at [Web of Science](https://images.webofknowledge.com/WOK46/help/WOS/h_fullrec.html).

## Publication years
```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(literature, aes(YearPublished)) +
  geom_histogram(binwidth = 1, fill = "darkgreen") +
  ggtitle("Year published") +
  xlab("Year") +
  ylab("Article count")

# Calculate relative publication counts
yearTable <- table(literature$YearPublished)    # Tabulate publication years
yearDF <- as.data.frame(yearTable)              # Turn to dataframe
names(yearDF) <- c("Year", "Freq")              # Fix column names

# Merge to dataframe of total publication numbers (years)
yearDF <- merge(yearDF, years, by.x = "Year", by.y = "Year", 
                all.x = TRUE)
yearDF$Year <- as.numeric(as.character(yearDF$Year))    # factor to numeric
# Calculate published articles per total articles by year
yearDF$Fraction <- yearDF$Freq / yearDF$Records
```

## Relative publication volume
```{r, echo=FALSE}
ggplot(yearDF, aes(Year, Fraction, group = 1)) +
  geom_line(color = "darkgreen") +
  xlab("Year") +
  ylab("Fraction of all publications") +
  ggtitle("Relative publication volume")
```


## Important authors
Sorted by the number of articles published and by the total number of citations.
```{r,echo=FALSE}
# Calculating total number of citations for each author
citationSums <- aggregate(literatureByAuthor$TimesCited,
    by = list(AuthorFullName = toupper(literatureByAuthor$AuthorFullName)),
    FUN = sum, na.rm = T)

# Fixing column names
names(citationSums) <- c("AuthorFullName", "TotalTimesCited")
# Crating new data frame to plot citations by author

# Extract author names
authorNames <- unlist(strsplit(literature$AuthorFullName, ";"))
# Remove apostrophes
authorNames <- gsub("'", "", authorNames)
# Count author name frequencies
authors <- table(authorNames)
# Transform to a data frame
authors <- as.data.frame(authors)
# Merge with data frame containing the total times citated by each author
authors <- merge(authors, citationSums, by.x = "authorNames",
              by.y = "AuthorFullName" )
# Fix column name
names(authors)[1] <- "AuthorFullName"
# Sort the table by total times sited, decreasing order
authors <- authors[with (authors, order(-TotalTimesCited)), ]

# Sort authors by number of articles, extract top 25,
# and reorder factors for plotting
authors <- authors[with (authors, order(-Freq)), ]
authorsPop <- head(authors, 25)
authorsPop <- transform(authorsPop, AuthorFullName = reorder(AuthorFullName, Freq))

ggplot(authorsPop, aes(AuthorFullName, Freq)) +
    geom_bar(stat = "identity", fill = "blue") +
    coord_flip() +
    ggtitle("Productive authors") +
    xlab("Author") +
    ylab("Number of articles")
```

```{r,echo=FALSE}
# Reorder AuthorFullName factor according to TotalTimesCited (decreasing order)
authors <- transform(authors,
                            AuthorFullName = reorder(AuthorFullName,
                                                     TotalTimesCited))

ggplot(head(authors, 25), aes(AuthorFullName, TotalTimesCited)) +
    geom_bar(stat = "identity", fill = "blue") +
    coord_flip() +
    ggtitle("Most cited authors") +
    xlab("Author") + ylab("Total times cited")
```

## Important publications
Sorted by number of published articles in the dataset and by the total number of citations.
```{r,echo=FALSE}
# Calculating total citations for each publication.
# Functionality same as for the authors, see above.

citationSums <- aggregate(literature$TimesCited,
    by = list(PublicationName= literature$PublicationName),
    FUN = sum, na.rm = T)
names(citationSums) <- c("PublicationName", "PublicationTotalCitations")
citationSums <- citationSums[with (citationSums, order(-PublicationTotalCitations)), ]
top25 <- head(citationSums[,c("PublicationName", "PublicationTotalCitations")], 25)
top25$PublicationName <- strtrim(top25$PublicationName, 50)

publications <- table(literature$PublicationName)
publications <- as.data.frame(publications)
names(publications) <- c("Publication", "Count")
publications <- publications[with (publications, order(-Count)), ]

publications$Publication <- strtrim(publications$Publication, 50)
publications <- transform(publications, Publication = reorder(Publication, Count))

literature <- merge(literature, citationSums,
                    by = "PublicationName" )

ggplot(head(publications, 25), aes(Publication, Count)) +
    geom_bar(stat = "identity", fill = "orange") +
    coord_flip() +
    theme(legend.position = "none") +
    ggtitle("Most popular publications") +
    xlab("Publication") +
    ylab("Article count")
```


```{r,echo=FALSE}
top25 <- transform(top25,
                          PublicationName = reorder(PublicationName,
                                                    PublicationTotalCitations))
ggplot(top25,
       aes(PublicationName, PublicationTotalCitations)) +
    geom_bar(stat = "identity", fill = "orange") +
    coord_flip() +
    theme(legend.position = "none") +
    ggtitle("Most cited publications") +
    xlab("Publication") + ylab("Total times cited")
```


## Important keywords
Sorted by the number of articles where the keyword is mentioned and by the total number of citations for the keyword.
```{r, echo=FALSE}
# Calculating total citations for each keyword
# Functionality same as for the authors, see above.

# Sometimes AuthorKeywords column is empty.
# Following if-else hack prevents crashing in those situations,
# either by using KeywordsPlus column or skipping keyword analysis.
if (using_KeywordsPlus == TRUE) {
  cat("No keywords. Using KeywordsPlus instead.")
  names(literature)[c(21, 22)] <- c("AuthorKeywordsTemp", "AuthorKeywords")
}

if (nrow(literatureByKeywords) == 0) {
  cat("No keywords")
} else {
  # Below functionality same as above for important authors.
  keywordCitationSum <- aggregate(literatureByKeywords$TimesCited,
                                by = list(AuthorKeywords =
                            literatureByKeywords$AuthorKeywords), FUN = sum,
                            na.rm = T)
  names(keywordCitationSum) <- c("AuthorKeywords", "TotalTimesCited")

  keywords <- unlist(strsplit(literature$AuthorKeywords, ";"))
  keywords <- trim(keywords)
  keywords <- table(keywords)
  keywords <- as.data.frame(keywords)
  names(keywords) <- c("AuthorKeywords", "Freq")

  keywords <- merge(keywords, keywordCitationSum, by = "AuthorKeywords")

  keywords <- keywords[with (keywords, order(-Freq)), ]
  keywordsPop <- head(keywords, 25)
  keywordsPop <- transform(keywordsPop, AuthorKeywords =
                             reorder(AuthorKeywords, Freq))

  ggplot(keywordsPop, aes(AuthorKeywords, Freq)) +
    geom_bar(stat = "identity", fill = "purple") +
    coord_flip() +
    ggtitle("Popular keywords") +
    xlab("Keyword") +
    ylab("Number of occurences")
}
```

```{r,echo=FALSE}
if (nrow(literatureByKeywords) > 0) {
  keywords <- keywords[with (keywords, order(-TotalTimesCited)), ]
  keywords <- transform(keywords, AuthorKeywords =
                             reorder(AuthorKeywords, TotalTimesCited))
  ggplot(head(keywords, 25), aes(AuthorKeywords, TotalTimesCited)) +
    geom_bar(stat = "identity", fill = "purple") +
    coord_flip()  +
    ggtitle("Most cited keywords") +
    xlab("Keyword") + ylab("Total times cited")
}

# Change the column names back to original ones
names(literature)[c(21, 22)] <- c("AuthorKeywords", "KeywordsPlus")
```

## Important papers
The most important papers and other sources are identified below using three importance measures: 1) in-degree in the citation network, 2) citation count provided by Web of Science (only for papers included in the dataset), and 3) PageRank score in the citation network. The top 25 highest scoring papers are identified using these measures separately. The results are then combined and duplicates are removed. Results are sorted by in-degree, and ties are first broken by citation count and then by the PageRank.

When a [Digital Object Identifier (DOI)](http://www.doi.org/) is available, the full paper can be found using [Resolve DOI](https://dx.doi.org/) website.

```{r,echo=FALSE}
# Create igraph
citationGraph <- graph.data.frame(citationEdges, vertices = citationNodes)
# Calculate PageRanks
citationNodes$PageRank <- page.rank(citationGraph)$vector
# Calculate in-degrees
citationNodes$InDegree <- degree(citationGraph, mode = "in")

# Extract the articles included in the data set and articles not included
# in the dataset
citationsLit <- citationNodes[citationNodes$Origin == "literature", ]
citationsRef <- citationNodes[citationNodes$Origin == "reference", ]
# Merge with selected columns in literature data frame
citationsLit <- merge(citationsLit,
                      literature[, c("ReferenceString",
                                     "DocumentTitle")],
                       by.x = "FullReference", by.y = "ReferenceString")
# Create article strings (document title, reference information and abstract
# separated by "|")
citationsLit$Article <- paste(toupper(citationsLit$DocumentTitle), " | ",
                              citationsLit$FullReference, " | ",
                                      citationsLit$Abstract)

# Trim FullReference to 100 characters
citationsLit$FullReference <- strtrim(citationsLit$FullReference, 100)
citationsRef$FullReference <- strtrim(citationsRef$FullReference, 100)
```

### Included in the dataset
These papers were included in the `r nrow(literature)` records downloaded from the Web of Science.
```{r, echo=FALSE}
# Sort citationsLit by TimesCited, decreasing
citationsLit <- citationsLit[with (citationsLit, order(-TimesCited)), ]
# Extract top 25
topLit <- head(citationsLit, 25)
# Sort by InDegree, decreasing
citationsLit <- citationsLit[with (citationsLit, order(-InDegree)), ]
# Add to list of top 25 most cited papers
topLit <- rbind(topLit, head(citationsLit, 25))
# Sort by PageRank, decreasing
citationsLit <- citationsLit[with (citationsLit, order(-PageRank)), ]
# Add to list of most cited and highest InDegree papers
topLit <- rbind(topLit, head(citationsLit, 25))
# Remove duplicates
topLit <- topLit[!duplicated(topLit[, "FullReference"]), ]
# Sort topLit by InDegree, break ties by TimesCited, then PageRank.
topLit <- topLit[with (topLit, order(-InDegree, -TimesCited, -PageRank)), ]
# Print list
kable(topLit[, c("Article", "InDegree", "TimesCited","PageRank")])
```

### Not included in the dataset
These papers and other references were not among the `r nrow(literature)` records downloaded from the Web of Science.
```{r, echo=FALSE}
# Sort citationsRef by InDegree, decreasing
citationsRef <- citationsRef[with (citationsRef, order(-InDegree)), ]
# Extract top 25
topRef <- head(citationsRef, 25)
# Sort by PageRank, decreasing
citationsRef <- citationsRef[with (citationsRef, order(-PageRank)), ]
# Add to list of highes in degree papers (references)
topRef <- rbind(topRef, head(citationsRef, 25))
# Remove duplicates
topRef <- topRef[!duplicated(topRef[, "FullReference"]), ]
# Sort by InDegree, break ties by PageRank
topRef <- topRef[with (topRef, order(-InDegree, -PageRank)), ]
# Print results
kable(topRef[, c("FullReference", "InDegree", "PageRank")])
```

## Most referenced publications
```{r, echo=FALSE}
references <- unlist(strsplit(literature$CitedReferences, ";"))

get_publication <- function(x) {
    publication <- "Not found"
    try(
        publication <- unlist(strsplit(x, ","))[[3]],
        silent = TRUE
    )
    return(publication)
}

refPublications <- sapply(references, get_publication)
refPublications <- sapply(refPublications, trim)
refPublications <- refPublications[refPublications != "Not found"]
refPublications <- as.data.frame(table(refPublications))
names(refPublications) <- c("Publication", "Count")
refPublications <- refPublications[with (refPublications, order(-Count)), ]

refPublications <- transform(refPublications,
                             Publication = reorder(Publication, Count))

ggplot(head(refPublications, 25), aes(Publication, Count)) +
    geom_bar(stat = "identity", fill = "orange") +
    coord_flip() +
    theme(legend.position = "none") +
    ggtitle("Most referenced publications") +
    xlab("Publication") +
    ylab("Count")
```

```{r, results='asis', echo=FALSE}
if (enableTM) {
cat('## Topic modeling output
[Topic modeling](https://en.wikipedia.org/wiki/Topic_model) is a type of statistical text mining method for discovering common "topics" that occur in a collection of documents. A topic modeling algorithm essentially looks through the abstracts included in the datasets for clusters of co-occurring of words and groups them together by a process of similarity.

The following columns describe each topic detected using [LDA topic modeling](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) by listing the ten most characteristic words in each topic. See also the [interactive visualization](output/topicmodelvis/index.html) for a better characterization of the topics and a visual representation of how (dis)similar the detected topics are to each other.

The number of topics is estimated using the [structural topic model library](https://cran.r-project.org/web/packages/stm/index.html) semantic coherence diagnostic values. Raw values are available in output file as kqualityvalues.csv and can be interpreted with [stm documentation](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) if necessary (see section 3.4). Search is limited between four and six topics for server performance reasons.
')
}
```

```{r, echo=FALSE}
if (enableTM) {
  tw <- data.frame(topickeywords)
  colnames(tw) <- gsub('X', 'Topic ', colnames(tw))
  kable(tw, col.names = colnames(tw))
}
```
